{"cells":[{"cell_type":"markdown","source":["Name: **Kartik More**<br>\n","Div: **BE09-R09**<br>\n","Roll no: **43149**<br>\n","Title: **Assignment 3**<br>"],"metadata":{"id":"t3XdpI2U_k8X"}},{"cell_type":"markdown","source":["*Problem Statement*:\n","\n","\n","    Build the Image classification model by dividing the model into following 4\n","    Stages:\n","\n","      1. Loading and preprocessing the image data\n","      2. Defining the model’s architecture\n","      3. Training the model\n","      4. Estimating the model’s performance\n","    "],"metadata":{"id":"Z6F6ZTcoD_Be"}},{"cell_type":"markdown","metadata":{"id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCV30xyVhFbE"},"outputs":[],"source":["import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"FIleuCAjoFD8","outputId":"44d8bbd7-1551-4ca3-ad75-d8c6d3644d0d","executionInfo":{"status":"ok","timestamp":1667355380473,"user_tz":-330,"elapsed":9,"user":{"displayName":"kartik m.","userId":"03070259464701780249"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.9.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["tf.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9ybg68L5_G0"},"outputs":[],"source":["# The convolutional network layer guives eyes to the AI (Deep learning model)\n","# We can take an image - A 3D frame as input as in input here.\n","# CNN will be able to visualize images just as humans do. We can also add memory to the AI.\n","\n","# Google developed Tensorflow\n","# Facebook deveoped Pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAWZP9mbAa89"},"outputs":[],"source":["# We have 4000 images of cats and dogs.\n","# So training set has a sum of 8000 images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-j9-ee8AmCn"},"outputs":[],"source":["# Test set has 1000 of each.\n","# So in all it has 2000 images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BeBkuGIBWmu","outputId":"9c342a8e-a5f2-44e9-e554-197029616c17","executionInfo":{"status":"ok","timestamp":1667355421642,"user_tz":-330,"elapsed":22540,"user":{"displayName":"kartik m.","userId":"03070259464701780249"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1LXMx0WB0rd","outputId":"481fdd22-66bf-4d1c-a4ee-6009b288a520","executionInfo":{"status":"ok","timestamp":1667355427686,"user_tz":-330,"elapsed":571,"user":{"displayName":"kartik m.","userId":"03070259464701780249"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /content/drive/MyDrive/Datasets/cats_dogs_data.zip, /content/drive/MyDrive/Datasets/cats_dogs_data.zip.zip or /content/drive/MyDrive/Datasets/cats_dogs_data.zip.ZIP.\n"]}],"source":["!unzip \"/content/drive/MyDrive/Datasets/cats_dogs_data.zip\""]},{"cell_type":"markdown","metadata":{"id":"oxQxCBWyoGPE"},"source":["## 1. Loading and pre-processing the image data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X6UzyUNGpVo"},"outputs":[],"source":["# Scott Overfitting : We get very high accuracy over the training set and very low on the testing set.\n","# We apply transformation to avoid overfitting.\n","# After those tranformations we get augmented images, this is known as IMAGE AUGMENTATION: we augment the variety, the diversity of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mx3CbHhHcd3"},"outputs":[],"source":["# Using keras we can do the following data preprcessing:\n","# Image\n","# Time series\n","# Text "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHOWONCMH0Uu"},"outputs":[],"source":["# ImageDataGenerator in Keras\n","# Generate batches of tensor image data with real-time data augmentation.\n","# We will create batches of 32 images which will be either the real one or the augmented ones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_qSRI0rISSZ"},"outputs":[],"source":["# We can try with other parameters as well, maybe we can get a better accuracy with them."]},{"cell_type":"markdown","metadata":{"id":"MvE-heJNo3GG"},"source":["### Preprocessing the Training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmBW5AXcJHFo"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","    rescale=1/255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n",")\n","\n","# This object of ImageDataGenerator represents the tools that will apply all the transformations\n","# on the images of the traning set\n","\n","# Rescale applies feature scaling to each and every single \n","# one of your pixels by dividing their value by 255.\n","# Each pixel takes value b/w 0 to 255.\n","# Hence by dividing by 255 we get values b/w 0 and 1 which is exactly like normalization.\n","# feature scaling is compulsory for training neural networks.\n","\n","# This ImageDataGenerator object is used for image augmentation to avoid data overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qjg3Tz6RK0VM","outputId":"77753861-52f2-40fb-a721-eab61a218076"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8000 images belonging to 2 classes.\n"]}],"source":["training_set = train_datagen.flow_from_directory(\n","    \"/content/dataset/training_set\", # path\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='binary'\n",")\n","# we will take the data in batches and resize in order to reduce the computations\n","#  of the image and make it less compute intensive.\n","\n","# target_size - Final size of your images that will be fed in the CNN.\n","# batch_size - 32 id the classic default value.\n","\n","# class_mode - can be binary or categorical"]},{"cell_type":"markdown","metadata":{"id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG07Xj9fMN4B"},"outputs":[],"source":["# fit_transform on training set.\n","\n","# but only fit in testing set as we want this to be totally new set for the model.\n","# To avoid information leakage from the test set\n","\n","\n","# Same thing hre, we will have to just scale the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHTZ3WLAMpLd"},"outputs":[],"source":["test_datagen = ImageDataGenerator(rescale=1/255)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJmfb6WBM0Cb","outputId":"dc554ecf-614d-449d-e771-c6b298770a0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2000 images belonging to 2 classes.\n"]}],"source":["testing_set = test_datagen.flow_from_directory(\n","    \"/content/dataset/test_set\", # path\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='binary'\n",")\n","\n","# taarget_size should be the same as mentioned for the training set."]},{"cell_type":"markdown","metadata":{"id":"af8O4l90gk7B"},"source":["## 2. Defining the model’s architecture\n"]},{"cell_type":"markdown","metadata":{"id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuPO_s-67NVT"},"outputs":[],"source":["# Sequence allows us to create an Artificial nn as a sequence of layers.\n","cnn = tf.keras.models.Sequential()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r3Neh_e7gFa"},"outputs":[],"source":["# We'll use the add methods to add new layers for each step"]},{"cell_type":"markdown","metadata":{"id":"u5YJj_XMl5LF"},"source":["### Step 1 - Convolution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2A4znzo78RY"},"outputs":[],"source":["cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=[64, 64, 3]))\n","\n","# filter - number of feature detectors that you want to apply to your image. Experiment for better results.\n","\n","# Kernel size - is the size of the feature detector, which is also the size of the number\n","# of columns of the square array(usually)\n","\n","# activation - is set to None by default, we want that to be rectifier function until the output layer.\n","\n","# **kwargs - input\n","# input_shape of the inputs\n","# We converted it to 64x64\n","# We are working on colored shape so input_shape will be in 3 dimention corresponding to RGB code of colors [64, 64, 3]\n","# If we are working with back and white image then it will be [64, 64, 1]\n"]},{"cell_type":"markdown","metadata":{"id":"tf87FpvxmNOJ"},"source":["### Step 2 - Pooling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvP3ENl_9y07"},"outputs":[],"source":["# We are using max pooling, you can use any pooling method.\n","\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","\n","# We get the maximum pixel of the frame(sub-matrix) of the Feature map(input) in the pooled feature map.\n","#  pool_size - is the size of that frame (Square matrix) 2 is recommended.\n","\n","# Stride - we will slide by these any pixels for the next output in the pooled matrix.\n","# Recommended as 2 to select all the pixels as frame size is 2.\n","\n","# padding - if it is \"valid\" then it will just ignore the other cells outside the frame.\n","# with \"same\" padding they'll just add extra columns with 0 pixels at the side and consider them. "]},{"cell_type":"markdown","metadata":{"id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wY2LEcA_UVi"},"outputs":[],"source":["# cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=[64, 64, 3]))\n","\n","# input_shape is used only when you add your first layer.\n","# Here we are on the second convoltional layer, so we can just remove the input_shape parameter\n","\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n","\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"]},{"cell_type":"markdown","metadata":{"id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnwWvPTr_3RZ"},"outputs":[],"source":["# Here we faltten all the results of the CNN into a single vector, which will become \n","# the input of a future fully connected network.\n","\n","cnn.add(tf.keras.layers.Flatten())\n","\n","# This class \"Faltten\" does not take any parameters"]},{"cell_type":"markdown","metadata":{"id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gbP5eYZAhrd"},"outputs":[],"source":["cnn.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n","\n","# Dense() class is used to add hidden neurons.\n","\n","# units - is the number of neurons.\n","\n","# It is reccomended to use the rectifier activation function until you reach the output layer."]},{"cell_type":"markdown","metadata":{"id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I659GlXuBDWh"},"outputs":[],"source":["# This final layer will still be conneced to the input layer.\n","# CAT OR DOG so units is 1.\n","\n","# For binry classification we use sigmoid.\n","# For multiple classification we will use softmax.\n","\n","cnn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"]},{"cell_type":"markdown","metadata":{"id":"D6XkI90snSDl"},"source":["## 3. Training the CNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BKA4HavBukV"},"outputs":[],"source":["# Until now we made the brain, now we will make the brain smart."]},{"cell_type":"markdown","metadata":{"id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GB2MUXICKGE"},"outputs":[],"source":["# optimizer, loss function, metrics\n","\n","# We aare doing binary classification.\n","\n","# Adam optimizer - perform stochastic gradient descent, update the weights in order to reduce the loss error\n","# etween the preictions anf the target.\n","\n","# loss because we are doing binary classification.\n","\n","# metrics - accuracy as we are doing binary classification, \n","# which is the most relevant way to measure the performance of a classificaiton model.\n","\n","cnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"]},{"cell_type":"code","source":["cnn.summary()"],"metadata":{"id":"BB2vTYD8IN9G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8968c6a6-d692-411c-d2c8-51ede65c832f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 32)        896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               802944    \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 813,217\n","Trainable params: 813,217\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LAVNA477B4J4","outputId":"8ecf5923-89d9-44cd-aa9e-88b0cf9c6014"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","250/250 [==============================] - 41s 130ms/step - loss: 0.6713 - accuracy: 0.5800 - val_loss: 0.6213 - val_accuracy: 0.6440\n","Epoch 2/25\n","250/250 [==============================] - 32s 129ms/step - loss: 0.5946 - accuracy: 0.6819 - val_loss: 0.5795 - val_accuracy: 0.6950\n","Epoch 3/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.5536 - accuracy: 0.7145 - val_loss: 0.5153 - val_accuracy: 0.7520\n","Epoch 4/25\n","250/250 [==============================] - 33s 131ms/step - loss: 0.5260 - accuracy: 0.7359 - val_loss: 0.5135 - val_accuracy: 0.7530\n","Epoch 5/25\n","250/250 [==============================] - 33s 133ms/step - loss: 0.5057 - accuracy: 0.7514 - val_loss: 0.4811 - val_accuracy: 0.7785\n","Epoch 6/25\n","250/250 [==============================] - 34s 135ms/step - loss: 0.4872 - accuracy: 0.7640 - val_loss: 0.4958 - val_accuracy: 0.7630\n","Epoch 7/25\n","250/250 [==============================] - 32s 130ms/step - loss: 0.4726 - accuracy: 0.7749 - val_loss: 0.4792 - val_accuracy: 0.7765\n","Epoch 8/25\n","250/250 [==============================] - 32s 130ms/step - loss: 0.4596 - accuracy: 0.7772 - val_loss: 0.4500 - val_accuracy: 0.7935\n","Epoch 9/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.4488 - accuracy: 0.7918 - val_loss: 0.4550 - val_accuracy: 0.7910\n","Epoch 10/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.4338 - accuracy: 0.7909 - val_loss: 0.4538 - val_accuracy: 0.7945\n","Epoch 11/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.4262 - accuracy: 0.7949 - val_loss: 0.4379 - val_accuracy: 0.7995\n","Epoch 12/25\n","250/250 [==============================] - 34s 135ms/step - loss: 0.4186 - accuracy: 0.8026 - val_loss: 0.4378 - val_accuracy: 0.8025\n","Epoch 13/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.4148 - accuracy: 0.8129 - val_loss: 0.4229 - val_accuracy: 0.8085\n","Epoch 14/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.3991 - accuracy: 0.8142 - val_loss: 0.4247 - val_accuracy: 0.8115\n","Epoch 15/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.4035 - accuracy: 0.8155 - val_loss: 0.4367 - val_accuracy: 0.8035\n","Epoch 16/25\n","250/250 [==============================] - 32s 127ms/step - loss: 0.3963 - accuracy: 0.8167 - val_loss: 0.4305 - val_accuracy: 0.8035\n","Epoch 17/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.3808 - accuracy: 0.8279 - val_loss: 0.4383 - val_accuracy: 0.8020\n","Epoch 18/25\n","250/250 [==============================] - 33s 133ms/step - loss: 0.3797 - accuracy: 0.8253 - val_loss: 0.4181 - val_accuracy: 0.8145\n","Epoch 19/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.3688 - accuracy: 0.8336 - val_loss: 0.4224 - val_accuracy: 0.8120\n","Epoch 20/25\n","250/250 [==============================] - 32s 129ms/step - loss: 0.3628 - accuracy: 0.8380 - val_loss: 0.4563 - val_accuracy: 0.7980\n","Epoch 21/25\n","250/250 [==============================] - 32s 127ms/step - loss: 0.3679 - accuracy: 0.8325 - val_loss: 0.4302 - val_accuracy: 0.8135\n","Epoch 22/25\n","250/250 [==============================] - 32s 127ms/step - loss: 0.3540 - accuracy: 0.8379 - val_loss: 0.4294 - val_accuracy: 0.8220\n","Epoch 23/25\n","250/250 [==============================] - 32s 127ms/step - loss: 0.3446 - accuracy: 0.8474 - val_loss: 0.4591 - val_accuracy: 0.8180\n","Epoch 24/25\n","250/250 [==============================] - 34s 135ms/step - loss: 0.3467 - accuracy: 0.8475 - val_loss: 0.4651 - val_accuracy: 0.7980\n","Epoch 25/25\n","250/250 [==============================] - 32s 128ms/step - loss: 0.3446 - accuracy: 0.8450 - val_loss: 0.4190 - val_accuracy: 0.8065\n"]}],"source":["# We are training and evaluating at the same time here, bacause we are doing a specific application - computer vision.\n","\n","histroy = cnn.fit(x = training_set, validation_data = testing_set, epochs=25)\n","\n","# 250*32 = 8000 total images.\n","# epochs were started from 10 to 15 to 25 when it was perfect."]},{"cell_type":"markdown","metadata":{"id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNtDCmjqE51K"},"outputs":[],"source":["import numpy as np\n","from keras.preprocessing import image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiAN1ZtyE5mR","outputId":"262071cc-3778-4db7-c3bb-d69ebf4b28c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 126ms/step\n","[[1.]]\n"]}],"source":["from tensorflow.python.framework.ops import reset_default_graph\n","from tensorflow.keras.utils import img_to_array\n","# Load the test image form our folder in PIL format.\n","\n","test_image = tf.keras.utils.load_img(\n","    \"/content/dataset/single_prediction/cat_or_dog_1.jpg\",\n","    target_size = (64, 64)\n",")\n","\n","# target_size - image should have the same size as we used in the training.\n","\n","# Convert that PIL format into a 2D array using image_to_array function in image module which is\n","# exactly the format of array expected by the predict function. We'll update this now.\n","\n","test_image = img_to_array(test_image)\n","# It needs to take that image in the PIL format that needs to be converted into numpy format.\n","\n","# Our images were trained in batches of 32, single images were not trained, so we have this extra\n","# dimension of the batch and we are about to deploy our model on a single image.\n","# We'll add an extraa fake dimension.\n","test_image = np.expand_dims(test_image, axis=0)\n","\n","# axis - where do we want to add this extra dimension. The dimension is always the first dimension,\n","# as you first give the batch of images and inside each batch you get the differnt images.\n","\n","result = cnn.predict(test_image)\n","\n","# The way to figure out what is cat or dog (0 or 1) is \n","# to call the class indices object from our training set object.\n","# print(training_set.class_indices)\n","# 1 - dog, 0 - cat\n","\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLqsSK6dIrM2"},"outputs":[],"source":["# training_set.class_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MI6RQBW5I_fg"},"outputs":[],"source":["if result[0][0] == 1:\n","  prediction = \"dog\"\n","else:\n","  prediction = \"cat\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ED9KB3I54c1i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4d28173-5128-4401-c417-a1777833f687"},"outputs":[{"output_type":"stream","name":"stdout","text":["dog\n"]}],"source":["print(prediction)"]},{"cell_type":"code","source":[],"metadata":{"id":"khdEu0PRL0Hp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}